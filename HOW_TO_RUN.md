# ğŸš€ PROJECT PLUTO - VISION-DRIVEN REFLEX AGENT - COMPLETE RUN GUIDE

## âš¡ QUICK START (Raspberry Pi)

### Step 1: Automated Setup
```bash
# Navigate to project
cd ~/pluto-voice-assistant

# Run setup script (installs everything: camera drivers, Python packages, models)
chmod +x setup_pi.sh
./setup_pi.sh
```

**This script installs:**
- rpicam-apps (camera support)
- libcamera-apps
- Python dependencies (OpenCV, Whisper, PyAudio, etc.)
- YuNet face detection model
- Piper TTS

### Step 2: Test Camera (CRITICAL!)
```bash
# Verify camera is working BEFORE starting Pluto
rpicam-hello --timeout 2000

# Expected: 2-second camera preview window
# If fails, see troubleshooting section below
```

### Step 3: Download Models

Models are mostly handled by `setup_pi.sh`, but verify:

#### A) YuNet Face Detection (~2.5MB)
```bash
# Auto-downloaded by setup_pi.sh, or manually:
python download_yunet_model.py

# Verify:
ls -lh models/face_detection_yunet_2023mar_int8bq.onnx
# Expected: ~2.5MB
```

#### B) Whisper STT (~39MB)
```bash
# Auto-downloaded on first run, or manually:
python -c "import whisper; whisper.load_model('tiny')"

# Cached in: ~/.cache/whisper/tiny.pt
```

#### C) Piper TTS Model (~63MB)
```bash
# Download from: https://github.com/rhasspy/piper/releases
# Download both files:
#   - en_US-lessac-medium.onnx
#   - en_US-lessac-medium.onnx.json

wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-lessac-medium.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-lessac-medium.onnx.json

# Place in models/ directory
mv en_US-lessac-medium.onnx* models/
```

#### D) Qwen2.5 LLM Model (~310MB)
```bash
# Install Ollama (if not done by setup_pi.sh):
curl -fsSL https://ollama.com/install.sh | sh

# Pull model:
ollama pull qwen2.5:0.5b-instruct-q4_k_M

# Verify:
ollama list
# Should show: qwen2.5:0.5b-instruct-q4_k_M
```

### Step 4: Verify Configuration
```bash
# Check models are in place:
ls -lh models/

# Expected output:
#   face_detection_yunet_2023mar_int8bq.onnx (~2.5MB)
#   en_US-lessac-medium.onnx (~63MB)
#   en_US-lessac-medium.onnx.json (~few KB)

# If paths differ, edit src/config.py:
#   VISION_CONFIG["model_path"]
#   PIPER_CONFIG["model_path"]
```

### Step 5: RUN PLUTO! ğŸ‘ï¸ğŸ‰
```bash
# Make sure:
# 1. Camera is working (rpicam-hello test passed)
# 2. Ollama server is running (ollama serve in separate terminal)
# 3. Microphone and speakers connected

# Start Pluto:
python run.py
```

**Expected Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ğŸª PROJECT PLUTO ğŸª                          â•‘
â•‘           Vision-Driven Reflex Agent Voice Assistant             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ‘ï¸  Vision Worker initializing...
âœ… Vision Worker initialized (YuNet loaded)
ğŸ¤ STT Worker initializing...
âœ… STT Worker initialized (Whisper tiny)
â¸ï¸  STT paused (waiting for face detection)
ğŸ§  LLM Worker initializing...
âœ… LLM Worker initialized (Qwen2.5:0.5b)
ğŸ”Š TTS Worker initializing...
âœ… TTS Worker initialized (Piper)

ğŸ‘ï¸  Waiting for someone to appear...
```

**When you step into camera view:**
```
ğŸ‘ï¸  Face detected at (120, 95, 80, 80)
ğŸ”’ Face locked (ID: face_001)
ï¿½ Greeting: "Hi there! How can I help you today?"
â–¶ï¸  STT resumed (listening for response)

ğŸ¤ Listening for speech...
```

---

## ğŸ”§ TROUBLESHOOTING

### Issue: "Camera not detected" ğŸ¥
```bash
# Check camera connection:
vcgencmd get_camera
# Expected: supported=1 detected=1

# If detected=0:
# 1. Enable camera in raspi-config
sudo raspi-config
# Interface Options â†’ Camera â†’ Enable â†’ Reboot

# 2. Check cable connection (yellow to yellow on board)

# 3. Test camera:
rpicam-hello --timeout 2000
# Should show 2-second preview

# For detailed troubleshooting, see VISION_SETUP.md
```

### Issue: "YuNet model not found"
```bash
# Download manually:
python download_yunet_model.py

# Verify location:
ls -lh models/face_detection_yunet_2023mar_int8bq.onnx
# Expected: ~2.5MB file

# Check config path:
grep "model_path" src/config.py | grep yunet
```

### Issue: "Vision worker fails but audio works"
**This is normal fallback behavior!**

Pluto automatically falls back to audio-only mode if vision fails. To intentionally disable vision:

```python
# Edit src/config.py:
VISION_CONFIG = {
    "greeting_enabled": False,  # Disables auto-greeting
    ...
}
```

Or comment out in `src/orchestrator.py`:
```python
# self.enable_vision = False  # Change to False
```

### Issue: "Cannot connect to Ollama"
```bash
# Start Ollama server in separate terminal:
ollama serve

# Test it's running:
curl http://localhost:11434/api/tags

# Check Ollama service:
systemctl status ollama  # If installed as service
```

### Issue: "Piper model not found"
```bash
# Download both files:
cd models/
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-lessac-medium.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-lessac-medium.onnx.json

# Verify both exist:
ls -lh en_US-lessac-medium.onnx*
```

### Issue: "No microphone detected"
```bash
# Check audio devices:
arecord -l

# Expected: List of USB/audio devices

# Test recording:
arecord -d 3 test.wav
aplay test.wav

# Install audio tools if missing:
sudo apt-get install alsa-utils
```

### Issue: "Whisper model download fails"
```bash
# Manually download:
python -c "import whisper; whisper.load_model('tiny')"

# Check cached model:
ls -lh ~/.cache/whisper/tiny.pt
# Expected: ~39MB

# If internet issues, download elsewhere and copy to cache
```

---

## ğŸ“Š RUNNING TESTS

```powershell
# Activate virtual environment first
.\venv\Scripts\Activate.ps1

# Run all tests:
pytest tests\test_integration.py -v

# Run specific test:
pytest tests\test_integration.py::TestMetricsLogger::test_metric_logging -v

# With coverage report:
pytest tests\test_integration.py --cov=src --cov-report=html
# Open htmlcov\index.html in browser
```

---

## ğŸ“ˆ VIEWING METRICS

After running conversations, check the logs:

```powershell
# List generated log files
ls logs\

# View CSV metrics (import in Excel or pandas)
Get-Content logs\metrics_*.csv | Select-Object -First 20

# View JSON metrics
Get-Content logs\metrics_*.json | ConvertFrom-Json

# View summary
Get-Content logs\summary_*.txt
```

**Analyze in Python:**
```python
import pandas as pd
import matplotlib.pyplot as plt

# Load metrics
df = pd.read_csv('logs/metrics_20241015_120000.csv')

# Plot latencies
stt = df[(df['component'] == 'stt') & (df['metric_type'] == 'latency')]
plt.plot(stt['timestamp'], stt['value'])
plt.title('STT Latency Over Time')
plt.ylabel('Latency (ms)')
plt.show()
```

---

## âš™ï¸ ADVANCED CONFIGURATION

### Customize Greeting Behavior
```python
# Edit src/config.py:
VISION_CONFIG = {
    "greeting_enabled": True,  # Set False to disable auto-greeting
    "greeting_message": "Hello! How may I assist you?",  # Custom greeting
    "greeting_cooldown": 15.0,  # Seconds between greetings (prevents spam)
    ...
}
```

### Adjust Face Detection Sensitivity
```python
VISION_CONFIG = {
    "confidence_threshold": 0.5,  # Lower = more sensitive (more false positives)
    "confidence_threshold": 0.8,  # Higher = less sensitive (may miss faces)
    "lock_threshold_frames": 5,  # More frames = more reliable lock (slower)
    "face_lost_timeout_frames": 20,  # More tolerance = stays locked longer
    ...
}
```

### Change Camera Resolution/FPS
```python
VISION_CONFIG = {
    "frame_width": 640,  # Higher resolution (more CPU usage)
    "frame_height": 480,
    "camera_fps": 15,  # Higher FPS (more CPU usage)
    "frame_skip": 1,  # Process every frame (no skipping)
    ...
}
```

### Speed Up LLM Responses
```python
OLLAMA_CONFIG = {
    "max_tokens": 50,  # Shorter responses (default: 100)
    "temperature": 0.5,  # More deterministic (default: 0.7)
    ...
}
```

### Use Faster TTS Model
Download smaller Piper model and update:
```python
PIPER_CONFIG = {
    "model_path": "./models/en_US-lessac-low.onnx",  # Faster, lower quality
    ...
}
```

### Disable Vision (Audio-Only Mode)
If you don't have a camera or want pure audio interaction:
```python
# Option 1: In src/config.py
VISION_CONFIG = {
    "greeting_enabled": False,
    ...
}

# Option 2: In src/orchestrator.py
class Orchestrator:
    def __init__(self, ...):
        self.enable_vision = False  # Change from True to False
```

---

## ğŸ›‘ STOPPING PLUTO

```powershell
# Press Ctrl+C in terminal where Pluto is running

# You'll see graceful shutdown:
# ğŸ›‘ Shutting down...
# ğŸ”„ Shutting down workers...
# ğŸ¤ STT Worker stopping...
# âœ… STT Worker stopped
# ğŸ§  LLM Worker stopping...
# âœ… LLM Worker stopped
# ğŸ”Š TTS Worker stopping...
# âœ… TTS Worker stopped
# ğŸ“Š Saving metrics...
# âœ… Metrics saved to: logs/
# ğŸª PLUTO SHUTDOWN COMPLETE

# Also stop Ollama server in other terminal:
# Ctrl+C in terminal running "ollama serve"
```

---

## ğŸ“± RECOMMENDED WORKFLOW

### Terminal 1: Ollama Server
```bash
ollama serve
# Keep this running
```

### Terminal 2: Pluto Application
```bash
cd ~/pluto-voice-assistant
source .venv/bin/activate
python run.py

# Step into camera view, talk to Pluto, press Ctrl+C when done
```

### Terminal 3: Monitoring (Optional)
```bash
# Watch logs in real-time
tail -f logs/metrics_*.csv

# Or watch vision events:
tail -f logs/metrics_*.txt | grep "Vision"
```

## ğŸ¤– Understanding the Reflex Agent

### State Machine Flow:
```
IDLE â†’ FACE_DETECTED â†’ LOCKED_IN â†’ GREETING â†’ LISTENING â†’ PROCESSING â†’ RESPONDING â†’ FACE_LOST â†’ IDLE
```

### Key Behaviors:
1. **Proactive Greeting**: Pluto detects you and initiates conversation
2. **Face Locking**: Locks onto one person, ignores background distractions
3. **Occlusion Tolerance**: Stays locked if you briefly move out of frame (< 1.5s)
4. **Auto Reset**: Returns to IDLE when you leave for > 1.5s
5. **STT Pause/Resume**: Microphone only active when face is locked (saves CPU)

### Debugging Vision:
```bash
# Enable vision debug mode in src/config.py:
VISION_CONFIG = {
    "debug": True,  # Prints detailed face detection info
    ...
}

# Watch vision worker logs:
python run.py 2>&1 | grep "Vision"
```

---

## âœ… VERIFICATION CHECKLIST

Before running, ensure:
- [ ] **Raspberry Pi Camera connected** and enabled (`vcgencmd get_camera` shows detected=1)
- [ ] **Camera test passed** (`rpicam-hello --timeout 2000` works)
- [ ] **Virtual environment activated** (`source .venv/bin/activate`)
- [ ] **Dependencies installed** (via `setup_pi.sh` or `pip install -r requirements.txt`)
- [ ] **YuNet model downloaded** (`models/face_detection_yunet_2023mar_int8bq.onnx` exists, ~2.5MB)
- [ ] **Whisper model cached** (`~/.cache/whisper/tiny.pt` exists, ~39MB)
- [ ] **Piper model downloaded** (`models/en_US-lessac-medium.onnx` + `.json` exist)
- [ ] **Ollama server running** (`ollama serve` in separate terminal)
- [ ] **Qwen2.5 model pulled** (`ollama list` shows `qwen2.5:0.5b-instruct-q4_k_M`)
- [ ] **Microphone connected** and working (`arecord -l` shows devices)
- [ ] **Speakers/headphones connected** (`aplay -l` shows devices)

---

## ğŸ¯ FIRST RUN EXPECTED OUTPUT

### Startup Sequence:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ğŸª PROJECT PLUTO ğŸª                          â•‘
â•‘           Vision-Driven Reflex Agent Voice Assistant             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸª PROJECT PLUTO - Configuration Summary
======================================================================

ğŸ“ Paths:
  Project Root: /home/pi/pluto-voice-assistant
  Models Dir:   /home/pi/pluto-voice-assistant/models
  Logs Dir:     /home/pi/pluto-voice-assistant/logs

ğŸ‘ï¸  Vision:
  Model: face_detection_yunet_2023mar_int8bq.onnx
  Resolution: 320x240 @ 10fps (effective: 5fps with frame_skip=2)
  Detection Threshold: 0.6
  Lock Threshold: 3 frames (~0.3s)

ğŸ¤ Audio:
  Sample Rate: 16000 Hz
  Channels: 1

ğŸ¤– Models:
  Vision: YuNet (INT8 quantized, 2.5MB)
  Whisper: tiny (39M params, ~150MB RAM, ~200ms latency)
  Ollama: qwen2.5:0.5b-instruct-q4_k_M (~2GB RAM)
  Piper: en_US-lessac-medium.onnx (~200MB RAM)

======================================================================

ğŸš€ Initializing workers...

ğŸ‘ï¸  Vision Worker initializing...
   Loading YuNet model from: models/face_detection_yunet_2023mar_int8bq.onnx
   Starting rpicam-vid: 320x240 @ 10fps YUV420
   Camera warmup...
âœ… Vision Worker initialized
   Vision warmup: Processing test frame...
   Vision warmup complete: 85ms

ğŸ¤ STT Worker initializing...
   Loading Whisper tiny model (cached)...
   Opening audio input: 16000Hz
âœ… STT Worker initialized
   STT warmup: Processing silent audio...
   STT warmup complete: 180ms
â¸ï¸  STT paused (waiting for face detection)

ğŸ§  LLM Worker initializing...
   Checking Ollama server at: http://localhost:11434
   Model 'qwen2.5:0.5b-instruct-q4_k_M' ready
âœ… LLM Worker initialized
   LLM warmup: Running test inference...
   LLM warmup complete: 1200ms

ğŸ”Š TTS Worker initializing...
   Piper model found: models/en_US-lessac-medium.onnx
âœ… TTS Worker initialized
   TTS warmup: Synthesizing test audio...
   TTS warmup complete: 350ms

âœ… All workers initialized successfully

======================================================================
ğŸ‘ï¸  PLUTO IS READY - Waiting for someone to appear...
   Press Ctrl+C to stop
======================================================================

ğŸ‘ï¸  Vision: Scanning at 8.2 FPS, no faces detected...
```

### Interaction Sequence (When You Appear):

```
ğŸ‘ï¸  Face detected at (120, 95, 80, 80) confidence=0.78
ğŸ‘ï¸  Face detected at (121, 96, 80, 80) confidence=0.81 (frame 2/3)
ğŸ‘ï¸  Face detected at (122, 96, 80, 80) confidence=0.79 (frame 3/3)
ğŸ”’ Face locked (ID: face_001)
ğŸ‰ Greeting: "Hi there! How can I help you today?"
  ğŸ‘ï¸  Vision trigger â†’ LLM: 0ms
  ğŸ§  LLM: 1100ms
  ğŸ”Š TTS: 420ms
  â±ï¸  total (greeting): 1520ms
â–¶ï¸  STT resumed (listening for response)

ğŸ¤ Listening for speech...

ğŸ™ï¸  Speech detected...
   ğŸ“ Recognized: "what time is it"
  ğŸ¤ STT: 180ms
   ğŸ¤” Thinking about: "what time is it"
   ğŸ’­ Response: "I'm sorry, I don't have access to real-time information..."
  ğŸ§  LLM: 950ms
   ğŸ—£ï¸  Speaking response...
  ğŸ”Š TTS: 380ms
  â±ï¸  total: 1510ms

ğŸ¤ Listening for speech...

<You leave the camera view>

ğŸ‘ï¸  Face lost (1/15 frames)
ğŸ‘ï¸  Face lost (5/15 frames)
ğŸ‘ï¸  Face lost (10/15 frames)
ğŸ‘ï¸  Face lost (15/15 frames) - unlocking
ğŸ”“ Face unlocked (ID: face_001) - person left
â¸ï¸  STT paused (no face detected)
ğŸ‘ï¸  Waiting for someone to appear...
```

---

## ğŸ”„ DEVELOPMENT WORKFLOW

### Making Code Changes
```powershell
# 1. Edit files in src/
# 2. Run tests
pytest tests\test_integration.py -v

# 3. Test manually
python run.py

# 4. Check metrics
Get-Content logs\summary_*.txt
```

### Adding New Features
See `DOCUMENTATION.md` for detailed guides on:
- Adding new metrics
- Adding new workers
- Modifying configuration
- Creating new tests

---

## ğŸ“š ADDITIONAL RESOURCES

- **QUICKSTART.md**: Fast setup guide for beginners
- **QUICK_START_PI.md**: Raspberry Pi-specific quick start
- **ARCHITECTURE.md**: 4-worker reflex agent design documentation
- **VISION_SETUP.md**: Comprehensive vision system guide (63KB, troubleshooting, tuning)
- **README.md**: Project overview and key features

---

## ğŸ‰ You're Ready to Run Pluto!

### Quick Reference:
**Command to start**: `python run.py`  
**Stop**: `Ctrl+C`  
**Check metrics**: `cat logs/summary_*.txt`  
**Test camera**: `rpicam-hello --timeout 2000`  
**Vision docs**: See `VISION_SETUP.md` for detailed troubleshooting

### What to Expect:
1. ğŸ‘ï¸ **System starts** - Camera activates, vision worker begins scanning
2. ğŸ” **You appear** - Face detected after 3 consecutive frames (~0.3s)
3. ğŸ”’ **Face locked** - Pluto locks onto you, ignores background
4. ğŸ‘‹ **Auto-greeting** - "Hi there! How can I help you today?"
5. ğŸ¤ **Listening** - STT activates, waiting for your response
6. ğŸ’¬ **Conversation** - Continue naturally
7. ğŸ˜´ **You leave** - After 1.5s, Pluto returns to IDLE

Good luck with your vision-driven reflex agent! ğŸš€ğŸ‘ï¸ğŸ¤–

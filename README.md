# ğŸª Project Pluto - Vision-Driven Reflex Agent Voice Assistant

> **Proactive, Face-Detecting Voice Assistant for Raspberry Pi**

## ğŸ¯ What is Pluto?

Pluto is a **reflex agent** voice assistant that combines:
- **YuNet Face Detection** (Vision - initiates interaction)
- **Whisper** (Speech-to-Text)
- **Qwen2.5:0.5b** via Ollama (Large Language Model)
- **Piper** (Text-to-Speech)

### Key Features

âœ¨ **Vision-Driven**: Detects faces and initiates conversations automatically  
ğŸ”’ **Person Locking**: Locks onto one person, ignores background distractions  
ğŸ¯ **Reflex Behavior**: Greets people proactively when detected  
ğŸš€ **Optimized for Pi**: Runs on Raspberry Pi 4 with camera support  
ğŸ“´ **Fully Offline**: All processing happens locally, no internet required

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pi Camera    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ (Video Stream)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YuNet Vision â”‚ â† vision_worker.py
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ (Face Detected!)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Greeting: "Hi there!"
â”‚ Orchestrator â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
       â”‚                                 â–¼
       â”‚ Activates                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                            â”‚   LLM   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
â”‚  Whisper STT â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                        â–¼
       â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Piper TTSâ”‚
                                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                        â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ Speaker â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§© Core Components

| Component | File | Responsibility |
|-----------|------|----------------|
| **Vision Worker** | `vision_worker.py` | Detects faces, locks onto people, triggers greetings |
| **STT Worker** | `stt_worker.py` | Captures audio, converts to text via Whisper |
| **LLM Worker** | `llm_worker.py` | Processes text, generates responses via Qwen2.5 |
| **TTS Worker** | `tts_worker.py` | Synthesizes speech from text via Piper |
| **Agent State** | `agent_state.py` | Manages conversation states and transitions |
| **Orchestrator** | `orchestrator.py` | Coordinates 4 workers, manages vision-driven logic |
| **Metrics Logger** | `metrics_logger.py` | Records performance data, generates reports |
| **Configuration** | `config.py` | Centralized settings management |

---

## ğŸ“ Project Structure

```
pluto/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vision_worker.py       # YuNet face detection
â”‚   â”‚   â”œâ”€â”€ stt_worker.py          # Whisper speech recognition
â”‚   â”‚   â”œâ”€â”€ llm_worker.py          # Ollama/Qwen2.5 inference
â”‚   â”‚   â””â”€â”€ tts_worker.py          # Piper text-to-speech
â”‚   â”œâ”€â”€ orchestrator.py            # 4-worker coordination + reflex logic
â”‚   â”œâ”€â”€ agent_state.py             # State machine for conversation flow
â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”œâ”€â”€ metrics_logger.py          # Performance tracking
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/                        # Model storage directory
â”‚   â”œâ”€â”€ face_detection_yunet_2023mar_int8bq.onnx  # YuNet face detector
â”‚   â”œâ”€â”€ whisper/                   # Whisper models (auto-downloaded)
â”‚   â””â”€â”€ piper/                     # Piper voices (download separately)
â”œâ”€â”€ logs/                          # Metric logs and reports
â”œâ”€â”€ tests/                         # Unit tests
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ download_yunet_model.py        # YuNet model downloader
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ setup_pi.sh                    # Raspberry Pi setup script
â”œâ”€â”€ run.py                         # Main entry point
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ QUICKSTART.md                  # 5-minute setup guide
â”œâ”€â”€ ARCHITECTURE.md                # System design documentation
â””â”€â”€ VISION_SETUP.md                # Vision system detailed guide
```

---

## ğŸš€ Quick Start

See **QUICKSTART.md** for detailed setup instructions.

### Prerequisites
- **Raspberry Pi 4** (4GB RAM recommended)
- **Raspberry Pi Camera** (libcamera-compatible)
- Python 3.8+
- Ollama installed and running
- Microphone and speakers

### Installation (Raspberry Pi)
```bash
cd ~/pluto
chmod +x setup_pi.sh
./setup_pi.sh  # Installs camera drivers, downloads models, sets up environment
python run.py
```

### What Happens When You Run?
1. ğŸ‘ï¸ **Vision system starts** - Camera activates, begins scanning for faces
2. ğŸ” **Detection** - When a face is detected, Pluto locks onto that person
3. ğŸ‘‹ **Greeting** - Pluto automatically says "Hi there! How can I help you today?"
4. ğŸ¤ **Listening** - STT activates, waiting for your response
5. ğŸ’¬ **Conversation** - Continue talking naturally
6. ğŸ˜´ **Reset** - When you leave, Pluto returns to IDLE and waits for the next person

---

## ğŸ“Š Performance Tracking

Every operation is measured and logged:
- ğŸ‘ï¸ **Vision**: Face detection FPS, face lock events
- ğŸ¤ **STT**: Whisper latency (100-200ms)
- ğŸ§  **LLM**: Qwen2.5 inference time (500-1500ms)
- ğŸ”Š **TTS**: Piper synthesis time (200-500ms)
- ğŸ’¾ **Memory**: Per-component usage (~3GB total on Pi 4)
- â±ï¸ **Total**: End-to-end conversation time

Results saved to `logs/` directory in CSV, JSON, and text formats.

### Typical Performance (Raspberry Pi 4)
- **Vision**: 5-10 FPS, 60-120ms per detection
- **STT**: 100-200ms for short phrases
- **LLM**: 500-1500ms depending on response complexity
- **TTS**: 200-500ms for typical responses
- **Total latency**: ~1-2 seconds from speech to audio response

---

## ğŸ“– Documentation

- **README.md** (this file) - Project overview
- **QUICKSTART.md** - Fast setup guide  
- **ARCHITECTURE.md** - 4-worker reflex agent design
- **VISION_SETUP.md** - Comprehensive vision system guide
- **HOW_TO_RUN.md** - Detailed runtime instructions

---

## ğŸ¤– Reflex Agent Behavior

Pluto follows a **state machine** for natural interaction:

```
IDLE â†’ FACE_DETECTED â†’ LOCKED_IN â†’ GREETING â†’ LISTENING â†’ PROCESSING â†’ RESPONDING â†’ FACE_LOST â†’ IDLE
```

**Key Behaviors:**
- ğŸ”’ **Face Locking**: Once locked onto a person, ignores other faces in background
- â±ï¸ **Loss Tolerance**: Allows 1.5 seconds of face absence before unlocking (handles brief occlusions)
- ğŸš« **Greeting Cooldown**: 10-second cooldown prevents repeated greetings to same person
- â¸ï¸ **STT Pause/Resume**: Microphone only activates after face is locked and greeting sent (saves CPU)

---

**ğŸª Project Pluto - Vision-driven reflex agent for proactive human-computer interaction**

*Built with reasoning and performance in mind.*
